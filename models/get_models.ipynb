{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7eoCFHWs0CI"
      },
      "source": [
        "\n",
        "# Models\n",
        "\n",
        "This notebooks serves as a centralized program to obtain some of the used in the rest of the repository.\n",
        "\n",
        "\n",
        "## Early Exit for Image Classification\n",
        "\n",
        "Among those models, we have made an implementation of Branchynet that we refer to as Early Exit. This is a technique that allows a model to terminate the inference at intermediate layers, potentially saving computation time and resources when a high confidence prediction can be made early on.\n",
        "\n",
        "The notebook demonstrates how to:\n",
        "\n",
        "- Implement Early Exit mechanisms within image classification models.\n",
        "- Convert TensorFlow models to TFLite format for deployment on CPU and TPU devices.\n",
        "- Convert models to ONNX format for use with Jetson devices.\n",
        "- Evaluate the performance of Early Exit models in terms of accuracy and computational efficiency.\n",
        "\n",
        " Due to the size of the ImageNet models, here you can also get the base models for the Jetson Orin Nano\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVIO2GyDyTTH"
      },
      "source": [
        "## Required packets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbHdT8y9Mnst"
      },
      "outputs": [],
      "source": [
        "!pip install -U tf2onnx onnx2pytorch  onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABbMNrCMy3TD"
      },
      "source": [
        "# Early Exit models for CIFAR-10\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYsa2Y9ozAvu",
        "outputId": "2a797173-3ea4-4962-acc0-4a2cc7cd04a7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "import time\n",
        "import keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "assert x_train.shape == (50000, 32, 32, 3)\n",
        "assert x_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BVIch1xzNRf"
      },
      "source": [
        "### EE_resnet8\n",
        "\n",
        "Build the models from TensorFlow and convert to TFLite to be used in CPU and TPU devices, and ONNX to be implemented in Jetson Devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlwTN7Ws9lye"
      },
      "outputs": [],
      "source": [
        "## TFLite\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE_threeLyaersResnet_500Epochs.h5\")\n",
        "model.trainable = False\n",
        "\n",
        "# Define common model\n",
        "common = Model(inputs=model.input, outputs=model.layers[18].output)\n",
        "\n",
        "# Define branch models\n",
        "branch1 = Model(inputs=model.layers[18].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[18].output, outputs=model.layers[-1].output)\n",
        "\n",
        "\n",
        "# Define custom layer to choose between branches\n",
        "class ChooseBranchLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(ChooseBranchLayer, self).__init__()\n",
        "        self.branch1 = branch1\n",
        "        self.branch2 = branch2\n",
        "\n",
        "    def call(self, inputs):\n",
        "        common_output = inputs\n",
        "        output1 = self.branch1(common_output)\n",
        "        condition = tf.reduce_max(output1) > 0.90\n",
        "        return tf.cond(condition, lambda: output1, lambda: self.branch2(common_output))\n",
        "\n",
        "\n",
        "# Create input layer\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "# Get common output\n",
        "common_output = common(inputs)\n",
        "\n",
        "# Use the custom layer to choose output based on condition\n",
        "final_output = ChooseBranchLayer()(common_output)\n",
        "\n",
        "\n",
        "# Define the new model\n",
        "model_EE= tf.keras.Model(inputs=inputs, outputs=final_output)\n",
        "\n",
        "\n",
        "spec = (tf.TensorSpec((None, 32, 32, 3), tf.float32, name=\"input\"),)\n",
        "#model_proto, _ = tf2onnx.convert.from_keras(model_EE, input_signature=spec, opset=13, output_path='EE_resnet8.onnx')\n",
        "\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_EE)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "with open(r\"EE_resnet8.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMdE4fLH81WC"
      },
      "source": [
        "For conversion to ONNX, we use PyTorch. Conversion from TFLite is possible with tf2onnx but we have experienced this pipeline produces a model that is not graph capturable and behaves very poorly in TensorRT. PyTorch produces a model that, when converted to ONNX, works correctly.\n",
        "\n",
        "There are two implementations: The first just returns the class prediction, which would be the model to use for production, and the second also includes a flag to track which branch was taken, to be used for testing. If you used TFLite, TensorRT will complain if the conditional block returns two outputs, so you would need to append the flag to the output tensor, so you have an N+1 vector in which the last element is the branch indicator. This model would produce a TensorRT engine that is not graph-capturable, as stated before.\n",
        "\n",
        "When using Pytorch, the model with the branch flag should be used for testing, but the model without the flag is better suited for production because we have observed a relatively high impact on performance when using the flag model.\n",
        "\n",
        "The thresholds here are arbitrary. You may refer to the paper to value the tradeoff between latency and accuracy and adjust to your needs. For testing, you can also assign 0 so that the branch is always taken, or 1 so that the branch is never taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCLDTDHPrTXP"
      },
      "outputs": [],
      "source": [
        "## ONNX model without branch indicator\n",
        "import tf2onnx\n",
        "import torch.onnx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import onnx\n",
        "from onnx2pytorch import ConvertModel\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, branch2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.common(input_tensor)\n",
        "        branch1_output = self.branch1(output)\n",
        "\n",
        "        # Find the maximum value in branch1_output\n",
        "        max_value = torch.max(branch1_output)\n",
        "\n",
        "        if max_value.item() > 0.9:\n",
        "            return branch1_output\n",
        "        else:\n",
        "            return self.branch2(output)\n",
        "\n",
        "model = ConditionalModel(common_model, branch1_model, branch2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(model, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_resnet8.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl_y_wLPPlH0",
        "outputId": "229bc02a-a523-45f4-a097-84a34b8114b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 84.34%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x_t = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaS4pvNDeXhm"
      },
      "outputs": [],
      "source": [
        "# Model with flag to track which branch was taken\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE_threeLyaersResnet_500Epochs.h5\")\n",
        "model.trainable = False\n",
        "\n",
        "common = Model(inputs=model.input, outputs=model.layers[18].output)\n",
        "branch1 = Model(inputs=model.layers[18].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[18].output, outputs=model.layers[-1].output)\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, branch2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.common(input_tensor)\n",
        "        branch1_output = self.branch1(output)\n",
        "\n",
        "        # Find the maximum value in branch1_output\n",
        "        max_value = torch.max(branch1_output)\n",
        "\n",
        "        if max_value.item() > 0.9:\n",
        "          return branch1_output, torch.tensor(0)\n",
        "        else:\n",
        "          return self.branch2(output), torch.tensor(1)\n",
        "\n",
        "\n",
        "model = ConditionalModel(common_model, branch1_model, branch2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(model, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_resnet8_flag.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne5gGmt9Zq1I",
        "outputId": "33c01fa5-3f41-45c7-af0c-d9275223248c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 84.34%\n",
            "Branch 0 taken: 5350 times\n",
            "Branch 1 taken: 4650 times\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x_t = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "branch = [0, 0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs, eexit = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        branch[eexit.item()] += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f\"Branch 0 taken: {branch[0]} times\")\n",
        "print(f\"Branch 1 taken: {branch[1]} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gNqAXgPEgjI"
      },
      "source": [
        "### Resnet56\n",
        "\n",
        "Same pipeline as with Resnet8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxUXHFQoElBA",
        "outputId": "ca8300ee-57e7-4774-e722-49155a618c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "66\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model(r\"EE_resnet56.h5\")\n",
        "\n",
        "# Preprocessing for Resnet56\n",
        "mean = [0.4914, 0.4822, 0.4465]\n",
        "std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = x / 255.\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train_p, y_train_p = prep(x_train, y_train)\n",
        "x_test_p, y_test_p = prep(x_test, y_test)\n",
        "\n",
        "target_layer_name = 're_lu_21'\n",
        "\n",
        "# Find the number of the layer\n",
        "layer_number = None\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if layer.name == target_layer_name:\n",
        "        layer_number = i\n",
        "        break\n",
        "\n",
        "print(layer_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzTyxbirE_gR"
      },
      "outputs": [],
      "source": [
        "## TFLite\n",
        "model = tf.keras.models.load_model(r\"EE_resnet56.h5\")\n",
        "common = Model(inputs=model.input, outputs=model.layers[3].output)\n",
        "branch1 = Model(inputs=model.layers[3].output, outputs=model.layers[-3].output)\n",
        "bb1 = Model(inputs=model.layers[3].output, outputs=model.layers[66].output)\n",
        "branch2 = Model(inputs=model.layers[66].output, outputs=model.layers[-2].output)\n",
        "bb2 = Model(inputs=model.layers[66].output, outputs=model.layers[-1].output)\n",
        "\n",
        "\n",
        "class ChooseBranchLayer1(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(ChooseBranchLayer1, self).__init__()\n",
        "        self.branch1 = branch1\n",
        "        self.bb1 = bb1\n",
        "        self.branch2 = branch2\n",
        "        self.bb2 = bb2\n",
        "    def call(self, inputs):\n",
        "        common_output = inputs\n",
        "        output1 = self.branch1(common_output)\n",
        "        output1 = tf.nn.softmax(output1)\n",
        "        condition = tf.reduce_max(output1) > 0.95\n",
        "        output = tf.cond(condition, lambda:[output1,0] , lambda: self.continuation(common_output))\n",
        "        return output\n",
        "        #return output1\n",
        "    def continuation(self,inputs):\n",
        "        common_output2 = self.bb1(inputs)\n",
        "        output2 = tf.nn.softmax(self.branch2(common_output2))\n",
        "        condition = tf.reduce_max(output2) > 0.95\n",
        "        return tf.cond(condition, lambda:[output2,1] , lambda: [tf.nn.softmax(self.bb2(common_output2)),2] )\n",
        "\n",
        "# Create input layer\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "# Get common output\n",
        "common_output = common(inputs)\n",
        "# Use the custom layer to choose output based on condition\n",
        "final_output, eexit = ChooseBranchLayer1()(common_output)\n",
        "# Define the new model\n",
        "EE_resnet56 = tf.keras.Model(inputs=inputs, outputs=[final_output, eexit])\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(EE_resnet56)\n",
        "tflite_model = converter.convert()\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "with open(r\"EE_resnet56.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNGl9F0YnXUK"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = len(y_test_p)\n",
        "\n",
        "# Preprocessing for Resnet56\n",
        "mean = [0.4914, 0.4822, 0.4465]\n",
        "std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = x / 255.\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_test_p, y_test_p = prep(x_test, y_test)\n",
        "\n",
        "x_test_p = x_test_p.astype('float32')\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], np.expand_dims(x_test_p[0], axis=0))\n",
        "interpreter.invoke()\n",
        "a = interpreter.get_tensor(output_details[0]['index'])\n",
        "b = interpreter.get_tensor(output_details[1]['index'])\n",
        "\n",
        "if isinstance(a, np.ndarray):\n",
        "  index = 0\n",
        "else:\n",
        "  index = 1\n",
        "\n",
        "\n",
        "for input, ground_truth in zip(x_test_p, y_test_p):\n",
        "    # Set the tensor to point to the input data to be inferred\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(input, axis=0))\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor\n",
        "    output_data = interpreter.get_tensor(output_details[index]['index'])\n",
        "\n",
        "    # Convert output data to predicted labels\n",
        "    predicted = np.argmax(output_data[0])\n",
        "\n",
        "    #print(f'\\noutput_data: {output_data}\\npredicted: {predicted}\\nground_truth: {ground_truth}\\n\\n')\n",
        "\n",
        "    if predicted == ground_truth[0]:\n",
        "      correct += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gocw2xf51F6s"
      },
      "outputs": [],
      "source": [
        "# ONNX without branch indicator\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE_resnet56.h5\")\n",
        "common = Model(inputs=model.input, outputs=model.layers[3].output)\n",
        "branch1 = Model(inputs=model.layers[3].output, outputs=model.layers[-3].output)\n",
        "bb1 = Model(inputs=model.layers[3].output, outputs=model.layers[66].output)\n",
        "branch2 = Model(inputs=model.layers[66].output, outputs=model.layers[-2].output)\n",
        "bb2 = Model(inputs=model.layers[66].output, outputs=model.layers[-1].output)\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "bb1_model, _ = tf2onnx.convert.from_keras(bb1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "bb2_model, _ = tf2onnx.convert.from_keras(bb2, opset=17)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, bb1_model, branch2_model, bb2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.bb1 = ConvertModel(bb1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "        self.bb2 = ConvertModel(bb2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        common_output = self.common(input_tensor)\n",
        "        output1 = self.branch1(common_output)\n",
        "        output1 = torch.nn.functional.softmax(output1, dim=1)\n",
        "\n",
        "        max_value = torch.max(output1)\n",
        "\n",
        "        if max_value.item() > 0.95:\n",
        "            return output1 # First branch exit\n",
        "        else:\n",
        "          common_output2 = self.bb1(common_output)\n",
        "          output2 = torch.nn.functional.softmax(self.branch2(common_output2), dim=1)\n",
        "\n",
        "          max_value = torch.max(output2)\n",
        "\n",
        "          if max_value.item() > 0.95:\n",
        "            return output2 # Second branch exit\n",
        "          else:\n",
        "            return torch.nn.functional.softmax(self.bb2(common_output2), dim=1) # Last exit through the main branch\n",
        "\n",
        "\n",
        "\n",
        "EE_model = ConditionalModel(common_model, branch1_model, bb1_model, branch2_model, bb2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(EE_model, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_resnet56.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O4bEjPpn-g7"
      },
      "source": [
        "The following blocks may be run if, with the previous model, you get an error message from TRT onnx parser like the following:\n",
        "\n",
        "\n",
        "\n",
        "```bash\n",
        "[E] [TRT] ModelImporter.cpp:768: While parsing node number 183 [Squeeze -> \"/bb2/Squeeze_model_14/global_average_pooling2d/Mean_Squeeze__1373:0/Squeeze_2_output_0\"]:\n",
        "[E] [TRT] ModelImporter.cpp:769: --- Begin node ---\n",
        "[E] [TRT] ModelImporter.cpp:770: input: \"/bb2/GlobalAveragePool_model_14/global_average_pooling2d/Mean:0/ReduceMean_output_0\"\n",
        "input: \"/bb2/Squeeze_model_14/global_average_pooling2d/Mean_Squeeze__1373:0/Unsqueeze_output_0\"\n",
        "output: \"/bb2/Squeeze_model_14/global_average_pooling2d/Mean_Squeeze__1373:0/Squeeze_2_output_0\"\n",
        "name: \"/bb2/Squeeze_model_14/global_average_pooling2d/Mean_Squeeze__1373:0/Squeeze_2\"\n",
        "op_type: \"Squeeze\"\n",
        "\n",
        "[E] [TRT] ModelImporter.cpp:771: --- End node ---\n",
        "[E] [TRT] ModelImporter.cpp:773: ERROR: ModelImporter.cpp:178 In function parseGraph:\n",
        "[6] Invalid Node - /bb2/Squeeze_model_14/global_average_pooling2d/Mean_Squeeze__1373:0/Squeeze_2\n",
        "Squeeze axes input must be an initializer! Try applying constant folding on the model using Polygraphy: https://github.com/NVIDIA/TensorRT/tree/master/tools/Polygraphy/examples/cli/surgeon/02_folding_constants\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6gwHAd6m823"
      },
      "outputs": [],
      "source": [
        "!export POLYGRAPHY_AUTOINSTALL_DEPS=1\n",
        "!pip install polygraphy\n",
        "!python3 -m pip install onnx_graphsurgeon --index-url https://pypi.ngc.nvidia.com\n",
        "!python3 -m pip install colored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idpSLFPZnEYC"
      },
      "outputs": [],
      "source": [
        "!polygraphy surgeon sanitize ../edge/Jetson_Nano/inference/models/EE_resnet56.onnx --fold-constants -o ../edge/Jetson_Nano/inference/models/EE_resnet56.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0DOHDLkZGas",
        "outputId": "f91dc95b-f0e4-4873-fd4e-1407069745ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.77%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x_t = torch.tensor(x_test_p, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test_p, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "EE_model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs = EE_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAmSyWEDKBat"
      },
      "outputs": [],
      "source": [
        "# ONNX with branch indicator\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE_resnet56.h5\")\n",
        "common = Model(inputs=model.input, outputs=model.layers[3].output)\n",
        "branch1 = Model(inputs=model.layers[3].output, outputs=model.layers[-3].output)\n",
        "bb1 = Model(inputs=model.layers[3].output, outputs=model.layers[66].output)\n",
        "branch2 = Model(inputs=model.layers[66].output, outputs=model.layers[-2].output)\n",
        "bb2 = Model(inputs=model.layers[66].output, outputs=model.layers[-1].output)\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "bb1_model, _ = tf2onnx.convert.from_keras(bb1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "bb2_model, _ = tf2onnx.convert.from_keras(bb2, opset=17)\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, bb1_model, branch2_model, bb2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.bb1 = ConvertModel(bb1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "        self.bb2 = ConvertModel(bb2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        common_output = self.common(input_tensor)\n",
        "        output1 = self.branch1(common_output)\n",
        "        output1 = torch.nn.functional.softmax(output1, dim=1)\n",
        "\n",
        "        max_value = torch.max(output1)\n",
        "\n",
        "        if max_value.item() > 0.95:\n",
        "            return output1, torch.tensor(0) # First branch exit\n",
        "        else:\n",
        "          common_output2 = self.bb1(common_output)\n",
        "          output2 = torch.nn.functional.softmax(self.branch2(common_output2), dim=1)\n",
        "\n",
        "          max_value = torch.max(output2)\n",
        "\n",
        "          if max_value.item() > 0.95:\n",
        "            return output2, torch.tensor(1) # Second branch exit\n",
        "          else:\n",
        "            output3 = torch.nn.functional.softmax(self.bb2(common_output2), dim=1)\n",
        "            return output3, torch.tensor(2) # Last exit through the main branch\n",
        "\n",
        "\n",
        "\n",
        "EE_model = ConditionalModel(common_model, branch1_model, bb1_model, branch2_model, bb2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(EE_model, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_resnet56_flag.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsrMMYNdjljA",
        "outputId": "36e8b590-7e27-4893-beda-1534344846e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.77%\n",
            "Branch 0 taken: 3381 times\n",
            "Branch 1 taken: 3486 times\n",
            "Branch 1 taken: 3133 times\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x_t = torch.tensor(x_test_p, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test_p, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "EE_model.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "branch = [0, 0, 0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs, eexit = EE_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        branch[eexit.item()] += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f\"Branch 0 taken: {branch[0]} times\")\n",
        "print(f\"Branch 1 taken: {branch[1]} times\")\n",
        "print(f\"Branch 2 taken: {branch[2]} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQRT5iC-rLnK"
      },
      "source": [
        "## Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJtwtCKirOY-"
      },
      "outputs": [],
      "source": [
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "\n",
        "# Preprocessing for AlexNet\n",
        "mean = [125.307, 122.95, 113.865]\n",
        "std = [62.9932, 62.0887, 66.7048]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train_p, y_train_p = prep(x_train, y_train)\n",
        "x_test_p, y_test_p = prep(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6-oH8UrkwD"
      },
      "outputs": [],
      "source": [
        "## TFLite\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE2_Alexnet_30Epochs.h5\")\n",
        "\n",
        "# Define common model\n",
        "common = Model(inputs=model.input, outputs=model.layers[6].output)\n",
        "\n",
        "# Define branch models\n",
        "branch1 = Model(inputs=model.layers[6].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[6].output, outputs=model.layers[-1].output)\n",
        "\n",
        "# Define custom layer to choose between branches\n",
        "class ChooseBranchLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(ChooseBranchLayer, self).__init__()\n",
        "        self.branch1 = branch1\n",
        "        self.branch2 = branch2\n",
        "\n",
        "    def call(self, inputs):\n",
        "        common_output = inputs\n",
        "        output1 = self.branch1(common_output)\n",
        "        condition = tf.reduce_max(output1) > 0.80\n",
        "        return tf.cond(condition, lambda: [output1,0], lambda: [self.branch2(common_output),1])\n",
        "\n",
        "# Create input layer\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "# Get common output\n",
        "common_output = common(inputs)\n",
        "\n",
        "# Use the custom layer to choose output based on condition\n",
        "final_output, eexit = ChooseBranchLayer()(common_output)\n",
        "\n",
        "# Define the new model\n",
        "model_EE = tf.keras.Model(inputs=inputs, outputs=[final_output, eexit])\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_EE)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "with open(r\"EE_alexnet.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqb6ihKseyF9",
        "outputId": "8661dd6e-2113-497f-80c9-9d325198366b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 76.41%\n"
          ]
        }
      ],
      "source": [
        "mean = [125.307, 122.95, 113.865]\n",
        "std = [62.9932, 62.0887, 66.7048]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train_p, y_train_p = prep(x_train, y_train)\n",
        "x_test_p, y_test_p = prep(x_test, y_test)\n",
        "\n",
        "x_test_p = x_test_p.astype('float32')\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], np.expand_dims(x_test_p[0], axis=0))\n",
        "interpreter.invoke()\n",
        "a = interpreter.get_tensor(output_details[0]['index'])\n",
        "b = interpreter.get_tensor(output_details[1]['index'])\n",
        "\n",
        "if isinstance(a, np.ndarray):\n",
        "  index = 0\n",
        "else:\n",
        "  index = 1\n",
        "\n",
        "correct = 0\n",
        "total = len(y_test_p)\n",
        "\n",
        "\n",
        "for input, ground_truth in zip(x_test_p, y_test_p):\n",
        "    # Set the tensor to point to the input data to be inferred\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(input, axis=0))\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output tensor\n",
        "    output_data = interpreter.get_tensor(output_details[index]['index'])\n",
        "\n",
        "    # Convert output data to predicted labels\n",
        "    predicted = np.argmax(output_data[0])\n",
        "\n",
        "    #print(f'\\noutput_data: {output_data}\\npredicted: {predicted}\\nground_truth: {ground_truth}\\n\\n')\n",
        "\n",
        "    if predicted == ground_truth[0]:\n",
        "      correct += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjygsfSLdBoD"
      },
      "outputs": [],
      "source": [
        "## ONNX model without branch indicator\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE2_Alexnet_30Epochs.h5\")\n",
        "common = Model(inputs=model.input, outputs=model.layers[6].output)\n",
        "branch1 = Model(inputs=model.layers[6].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[6].output, outputs=model.layers[-1].output)\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "\n",
        "\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, branch2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.common(input_tensor)\n",
        "        branch1_output = self.branch1(output)\n",
        "\n",
        "        # Find the maximum value in branch1_output\n",
        "        max_value = torch.max(branch1_output)\n",
        "\n",
        "        if max_value.item() > .8:\n",
        "            return branch1_output\n",
        "        else:\n",
        "            return self.branch2(output)\n",
        "\n",
        "model_EE = ConditionalModel(common_model, branch1_model, branch2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(model_EE, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_alexnet.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqV18YIJdryv",
        "outputId": "6673bff8-963b-4339-90e2-598cb4f25d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 76.41%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Preprocessing for AlexNet\n",
        "mean = [125.307, 122.95, 113.865]\n",
        "std = [62.9932, 62.0887, 66.7048]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train_p, y_train_p = prep(x_train, y_train)\n",
        "x_test_p, y_test_p = prep(x_test, y_test)\n",
        "\n",
        "x_t = torch.tensor(x_test_p, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test_p, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model_EE.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs = model_EE(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byFqItYedjhx"
      },
      "outputs": [],
      "source": [
        "# Model with flag to track which branch was taken\n",
        "\n",
        "model = tf.keras.models.load_model(r\"EE2_Alexnet_30Epochs.h5\")\n",
        "common = Model(inputs=model.input, outputs=model.layers[6].output)\n",
        "branch1 = Model(inputs=model.layers[6].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[6].output, outputs=model.layers[-1].output)\n",
        "\n",
        "common_model, _ = tf2onnx.convert.from_keras(common, opset=17)\n",
        "branch1_model, _ = tf2onnx.convert.from_keras(branch1, opset=17)\n",
        "branch2_model, _ = tf2onnx.convert.from_keras(branch2, opset=17)\n",
        "\n",
        "\n",
        "class ConditionalModel(nn.Module):\n",
        "    def __init__(self, common_model, branch1_model, branch2_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # Integrate ONNX models as submodules (adjust naming if needed)\n",
        "        self.common = ConvertModel(common_model)\n",
        "        self.branch1 = ConvertModel(branch1_model)\n",
        "        self.branch2 = ConvertModel(branch2_model)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        output = self.common(input_tensor)\n",
        "        branch1_output = self.branch1(output)\n",
        "\n",
        "        # Find the maximum value in branch1_output\n",
        "        max_value = torch.max(branch1_output)\n",
        "\n",
        "        if max_value.item() > 0.8:\n",
        "            return branch1_output, torch.tensor(0)\n",
        "        else:\n",
        "            return self.branch2(output), torch.tensor(1)\n",
        "\n",
        "model_EE = ConditionalModel(common_model, branch1_model, branch2_model)\n",
        "\n",
        "dummy_input = torch.randn(1, 32, 32, 3)  # Batch size of 1, 3 channels, 32x32 image\n",
        "torch.onnx.export(model_EE, dummy_input, \"../edge/Jetson_Nano/inference/models/EE_alexnet_flag.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9h_K5IbsZVZ",
        "outputId": "fd081bad-7d15-4a31-fed8-37926d54b395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 76.41%\n",
            "Branch 0 taken: 6138 times\n",
            "Branch 1 taken: 3862 times\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Preprocessing for AlexNet\n",
        "mean = [125.307, 122.95, 113.865]\n",
        "std = [62.9932, 62.0887, 66.7048]\n",
        "\n",
        "def prep(x, y):\n",
        "    x = (x - mean) / std\n",
        "    return x, y\n",
        "\n",
        "\n",
        "x_train_p, y_train_p = prep(x_train, y_train)\n",
        "x_test_p, y_test_p = prep(x_test, y_test)\n",
        "\n",
        "x_t = torch.tensor(x_test_p, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_test_p, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(x_t, y_t)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model_EE.eval()\n",
        "\n",
        "correct, total = 0, 0\n",
        "branch = [0, 0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in data_loader:\n",
        "        outputs, eexit = model_EE(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        branch[eexit.item()] += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f\"Branch 0 taken: {branch[0]} times\")\n",
        "print(f\"Branch 1 taken: {branch[1]} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJiyQ1CdAAxS"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(r\"EE2_Alexnet_30Epochs.h5\")\n",
        "\n",
        "# Define common model\n",
        "common = Model(inputs=model.input, outputs=model.layers[6].output)\n",
        "\n",
        "# Define branch models\n",
        "branch1 = Model(inputs=model.layers[6].output, outputs=model.layers[-2].output)\n",
        "branch2 = Model(inputs=model.layers[6].output, outputs=model.layers[-1].output)\n",
        "\n",
        "# Define custom layer to choose between branches\n",
        "class ChooseBranchLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(ChooseBranchLayer, self).__init__()\n",
        "        self.branch1 = branch1\n",
        "        self.branch2 = branch2\n",
        "\n",
        "    def call(self, inputs):\n",
        "        common_output = inputs\n",
        "        output1 = self.branch1(common_output)\n",
        "        condition = tf.reduce_max(output1) > 0.80\n",
        "        return tf.cond(condition, lambda: [output1,0], lambda: [self.branch2(common_output),1])\n",
        "\n",
        "# Create input layer\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "# Get common output\n",
        "common_output = common(inputs)\n",
        "\n",
        "# Use the custom layer to choose output based on condition\n",
        "final_output = ChooseBranchLayer()(common_output)\n",
        "\n",
        "# Define the new model\n",
        "model_EE = tf.keras.Model(inputs=inputs, outputs=final_output)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_EE)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "\n",
        "with open(r\"EE_alexnet.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx-4PEcuA_T0",
        "outputId": "04d57590-c00e-439c-c2b2-2c3e2d2aa021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 12.84%\n",
            "Branch 0 taken: 9892 times\n",
            "Branch 1 taken: 108 times\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Prepare the interpreter\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensor indices\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "def run_inference(image):\n",
        "    interpreter.set_tensor(input_details[0]['index'], image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    eexit = interpreter.get_tensor(output_details[1]['index'])\n",
        "    return output, eexit\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "branches = [0, 0]\n",
        "\n",
        "for i in range(len(x_test_p)):\n",
        "    # Prepare input data\n",
        "    input_data = np.expand_dims(x_test_p[i], axis=0).astype(np.float32)\n",
        "\n",
        "    # Run inference\n",
        "    eexit, output = run_inference(input_data)\n",
        "\n",
        "    # Convert output to class prediction\n",
        "    predicted_class = np.argmax(output[0])\n",
        "    true_class = np.argmax(y_test_p[i])\n",
        "\n",
        "    branches[int(eexit)] += 1\n",
        "\n",
        "    # Update the accuracy metrics\n",
        "    if predicted_class == true_class:\n",
        "        correct_predictions += 1\n",
        "    total_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f\"Branch 0 taken: {branches[0]} times\")\n",
        "print(f\"Branch 1 taken: {branches[1]} times\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e30oN2BZWcH7"
      },
      "source": [
        "# Imagenet\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU1J199PGopR"
      },
      "source": [
        "## Get base models\n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "The models are available with pretrained weights in `torchvision`. However, they are in Pytorch and for Raspberry and Coral Micro, we need to convert them to TensorFlow. Surprisingly, despite been at the moment the two most used frameworks, there are not many reliable libraries to directly convert from one to the other. What we have found to be the most reliable way to port them is to create the architecture in TensorFlow and manually copy the weights from the Pytorch model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy9heWyMGwRx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dropout, Flatten\n",
        "from torchsummary import summary\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2WYt4_SLZLM"
      },
      "source": [
        "#### Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri6Ues2DLWFY"
      },
      "outputs": [],
      "source": [
        "kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n",
        "\n",
        "def conv3x3(x, out_planes, stride=1, name=None):\n",
        "    x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n",
        "    return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n",
        "\n",
        "def basic_block(x, planes, stride=1, downsample=None, name=None):\n",
        "    identity = x\n",
        "\n",
        "    out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n",
        "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n",
        "    out = layers.ReLU(name=f'{name}.relu1')(out)\n",
        "\n",
        "    out = conv3x3(out, planes, name=f'{name}.conv2')\n",
        "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n",
        "\n",
        "    if downsample is not None:\n",
        "        for layer in downsample:\n",
        "            identity = layer(identity)\n",
        "\n",
        "    out = layers.Add(name=f'{name}.add')([identity, out])\n",
        "    out = layers.ReLU(name=f'{name}.relu2')(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "def make_layer(x, planes, blocks, stride=1, name=None):\n",
        "    downsample = None\n",
        "    inplanes = x.shape[3]\n",
        "    if stride != 1 or inplanes != planes:\n",
        "        downsample = [\n",
        "            layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n",
        "            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n",
        "        ]\n",
        "\n",
        "    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n",
        "    for i in range(1, blocks):\n",
        "        x = basic_block(x, planes, name=f'{name}.{i}')\n",
        "\n",
        "    return x\n",
        "\n",
        "def resnet(x, blocks_per_layer, num_classes=1000):\n",
        "    x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n",
        "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n",
        "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n",
        "    x = layers.ReLU(name='relu1')(x)\n",
        "    x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n",
        "    x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n",
        "\n",
        "    x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n",
        "    x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n",
        "    x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n",
        "    x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D(name='avgpool')(x)\n",
        "    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n",
        "    x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def resnet18(x, **kwargs):\n",
        "    return resnet(x, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "torch_model = models.resnet18(pretrained=True)\n",
        "torch_model.eval()\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "outputs = resnet18(inputs)\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SXDRXpXLp3I"
      },
      "outputs": [],
      "source": [
        "# Load pytorch weights\n",
        "state_dict = torch_model.state_dict()\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, layers.Conv2D):\n",
        "        layer.set_weights([state_dict[f'{layer.name}.weight'].numpy().transpose((2, 3, 1, 0))])\n",
        "    elif isinstance(layer, layers.Dense):\n",
        "        layer.set_weights([\n",
        "            state_dict[f'{layer.name}.weight'].numpy().transpose(),\n",
        "            state_dict[f'{layer.name}.bias'].numpy()\n",
        "        ])\n",
        "    elif isinstance(layer, layers.BatchNormalization):\n",
        "        keys = ['weight', 'bias', 'running_mean', 'running_var']\n",
        "        layer.set_weights([state_dict[f'{layer.name}.{key}'].numpy() for key in keys])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljAFqu7xLtVH",
        "outputId": "08846feb-4dcd-4984-931f-2160a2f42e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.5762787e-06\n"
          ]
        }
      ],
      "source": [
        "# Compare outputs\n",
        "input_batch = np.random.rand(1, 224, 224, 3).astype(model.dtype)\n",
        "output = model(input_batch).numpy()\n",
        "with torch.no_grad():\n",
        "    torch_output = torch_model(torch.tensor(input_batch.transpose((0, 3, 1, 2)))).numpy()\n",
        "print(np.abs(output - torch_output).max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtXkevrWLxNp"
      },
      "outputs": [],
      "source": [
        "model.save('resnet18_imagenet.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('resnet18_tflite.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "\n",
        "# Pytorch -> ONNX\n",
        "torch.onnx.export(torch_model, torch.tensor(input_batch.transpose((0, 3, 1, 2))), \"../edge/Jetson_Nano/inference/models/resnet18_imagenet.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiBWHqsxNTiy"
      },
      "source": [
        "#### Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W6aTKqtNVHx"
      },
      "outputs": [],
      "source": [
        "alexnet_pytorch = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "alexnet_pytorch.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CA4suY4NZ9J",
        "outputId": "e84aff74-0ffd-45f9-b62c-192952a76d83"
      },
      "outputs": [],
      "source": [
        "state_dict = alexnet_pytorch.state_dict()\n",
        "state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzyY0AksNcsx"
      },
      "outputs": [],
      "source": [
        "def alexnet(img_input, classes=1000):\n",
        "  # 1st conv layer\n",
        "  x = layers.ZeroPadding2D(padding=2)(img_input)\n",
        "  x = Conv2D(64, (11, 11), strides=(4, 4), padding='valid',\n",
        "             activation='relu', kernel_initializer='uniform', use_bias = True)(x)  # valid\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=(\n",
        "      2, 2), padding='valid', data_format='channels_last')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  # 2nd conv layer\n",
        "  x = layers.ZeroPadding2D(padding=2)(x)\n",
        "  x = Conv2D(192, (5, 5), strides=(1, 1), padding='valid',\n",
        "             activation='relu', kernel_initializer='uniform', use_bias = True)(x)\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=(\n",
        "      2, 2), padding='valid', data_format='channels_last')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  # 3rd conv layer\n",
        "  x = layers.ZeroPadding2D(padding=1)(x)\n",
        "  x = Conv2D(384, (3, 3), strides=(1, 1), padding='valid',\n",
        "             activation='relu', kernel_initializer='uniform', use_bias = True)(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  # 4th conv layer\n",
        "  x = layers.ZeroPadding2D(padding=1)(x)\n",
        "  x = Conv2D(256, (3, 3), strides=(1, 1), padding='valid',\n",
        "             activation='relu', kernel_initializer='uniform', use_bias = True)(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  # 5th conv layer\n",
        "  x = layers.ZeroPadding2D(padding=1)(x)\n",
        "  x = Conv2D(256, (3, 3), strides=(1, 1), padding='valid',\n",
        "             activation='relu', kernel_initializer='uniform', use_bias = True)(x)\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=(\n",
        "      2, 2), padding='valid', data_format='channels_last')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  #x = tf.keras.layers.AveragePooling2D(\n",
        "    #1,\n",
        "    #strides=1,\n",
        "    #padding='valid',\n",
        "    #data_format='channels_last'\n",
        "    #)(x)\n",
        "  #x = tf.reshape(x,[1,256,6,6])\n",
        "  # flattening before sending to fully connected layers\n",
        "  x = tf.keras.layers.Permute((3,1,2))(x)\n",
        "  #x  = tf.transpose(x, perm=[0,3,1,2])\n",
        "  print(x.shape)\n",
        "  x = Flatten(data_format = 'channels_last')(x)\n",
        "  print(x.shape)\n",
        "  # fully connected layers\n",
        "  x = Dense(4096, activation='relu', use_bias = True)(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "  x = Dense(4096, activation='relu', use_bias = True)(x)\n",
        "  #x = Dropout(0.5)(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "\n",
        "  # output layer\n",
        "  out = Dense(1000, use_bias = True)(x)#, activation='softmax')(x)\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZYYGo6RNjNv",
        "outputId": "9ae52d02-1153-4e7d-92bb-c98aca094576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 256, 6, 6)\n",
            "(None, 9216)\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(224, 224, 3))\n",
        "outputs = alexnet(inputs)\n",
        "alexnet_tf = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnfn3IhvNmtl"
      },
      "source": [
        "Weight loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx7eu9k_NpEJ"
      },
      "outputs": [],
      "source": [
        "alexnet_tf.layers[2].set_weights([state_dict['features.0.weight'].numpy().transpose((2, 3, 1, 0)),\n",
        "                                  state_dict['features.0.bias'].numpy()])\n",
        "alexnet_tf.layers[5].set_weights([state_dict['features.3.weight'].numpy().transpose((2, 3, 1, 0)),\n",
        "                                  state_dict['features.3.bias'].numpy()])\n",
        "alexnet_tf.layers[8].set_weights([state_dict['features.6.weight'].numpy().transpose((2, 3, 1, 0)),\n",
        "                                  state_dict['features.6.bias'].numpy()])\n",
        "alexnet_tf.layers[10].set_weights([state_dict['features.8.weight'].numpy().transpose((2, 3, 1, 0)),\n",
        "                                   state_dict['features.8.bias'].numpy()])\n",
        "alexnet_tf.layers[12].set_weights([state_dict['features.10.weight'].numpy().transpose((2, 3, 1, 0)),\n",
        "                                   state_dict['features.10.bias'].numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdfBJpmGNros"
      },
      "outputs": [],
      "source": [
        "alexnet_tf.layers[16].set_weights([\n",
        "            state_dict['classifier.1.weight'].numpy().transpose(),\n",
        "            state_dict['classifier.1.bias'].numpy()])\n",
        "alexnet_tf.layers[17].set_weights([\n",
        "            state_dict['classifier.4.weight'].numpy().transpose(),\n",
        "            state_dict['classifier.4.bias'].numpy()])\n",
        "alexnet_tf.layers[18].set_weights([\n",
        "            state_dict['classifier.6.weight'].numpy().transpose(),\n",
        "            state_dict['classifier.6.bias'].numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvkfgF36NzvI"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsjZtrbcN01Q",
        "outputId": "6c8bbd04-8c13-4e83-afe3-deaee3dba4a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "input_batch = np.random.rand(1, 224, 224, 3).astype(alexnet_tf.dtype)\n",
        "\n",
        "\n",
        "intermediate_outputs = []\n",
        "def hook(module, input, output):\n",
        "    intermediate_outputs.append(output)\n",
        "\n",
        "# Register hook to the desired intermediate layer\n",
        "\n",
        "#desired_layer = alexnet_pytorch.features.0\n",
        "hook_handle = alexnet_pytorch.classifier[6].register_forward_hook(hook)\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    alexnet_pytorch(torch.tensor(input_batch.transpose((0, 3, 1, 2))))\n",
        "\n",
        "# Access intermediate outputs\n",
        "hook_handle.remove()\n",
        "print(len(intermediate_outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPOlghAXN54P",
        "outputId": "8bc2f7a6-83d9-41f9-c96c-8ba917ac950e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1000])\n",
            "(1, 1000)\n",
            "1.8775463e-06\n"
          ]
        }
      ],
      "source": [
        "print(intermediate_outputs[0].shape)\n",
        "\n",
        "outputs1 = alexnet_tf.layers[18].output\n",
        "parcial = Model(inputs, outputs1)\n",
        "output1 = parcial(input_batch).numpy()\n",
        "print(output1.shape)\n",
        "\n",
        "print(np.abs(output1- intermediate_outputs[0].numpy()).max())#.transpose(0,3,1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnhY7AkfOB43"
      },
      "source": [
        "Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyktJevNOBRp"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "alexnet_tf.save('alexnet_imagenet.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(alexnet_tf)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('alexnet_tflite_def.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Pytorch -> ONNX\n",
        "alexnet_pytorch.fc = nn.Sequential(\n",
        "    *alexnet_pytorch.fc,\n",
        "    nn.Softmax(),\n",
        ")\n",
        "torch.onnx.export(alexnet_pytorch, torch.tensor(input_batch.transpose((0, 3, 1, 2))), \"../edge/Jetson_Nano/inference/models/alexnet_imagenet.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuJhQZKtGw8w"
      },
      "source": [
        "#### Resnet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fmu4DsGGzs4"
      },
      "outputs": [],
      "source": [
        "torch_model = models.resnet50(pretrained=True)\n",
        "torch_model.eval()\n",
        "\n",
        "summary(torch_model, (3, 224,224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-3dvyuXSY8d"
      },
      "outputs": [],
      "source": [
        "def residual_block_v1(\n",
        "    x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n",
        "\n",
        "    bn_axis = 3\n",
        "    if conv_shortcut:\n",
        "        shortcut = layers.Conv2D(4 * filters, 1, strides=stride, name=name + \".downsample.0\", use_bias = False)(x)\n",
        "        shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.00e-5, momentum = 0.9, name=name + \".downsample.1\")(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(filters, 1, strides=1, name=name + \".conv1\", use_bias = False)(x) # stride was = stride\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.00e-5, momentum= 0.9, name=name + \".bn1\")(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_1_relu\")(x)\n",
        "\n",
        "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name+\".pad2\")(x)\n",
        "    x = layers.Conv2D(filters, kernel_size, strides = stride, padding=\"valid\", name=name + \".conv2\", use_bias = False)(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.00e-5, momentum = 0.9, name=name + \".bn2\")(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_2_relu\")(x)\n",
        "\n",
        "    x = layers.Conv2D(4 * filters, 1, strides = 1, name=name + \".conv3\", use_bias = False)(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.00e-5,momentum = 0.9, name=name + \".bn3\")(x)\n",
        "\n",
        "    x = layers.Add(name=name + \"_add\")([shortcut, x])\n",
        "    x = layers.Activation(\"relu\", name=name + \".relu\")(x)\n",
        "    return x\n",
        "\n",
        "def stack_residual_blocks_v1(x, filters, blocks, stride1=2, name=None):\n",
        "    x = residual_block_v1(x, filters, stride=stride1, name=name + \".0\")\n",
        "    for i in range(1, blocks):\n",
        "        x = residual_block_v1(x, filters, conv_shortcut=False, name=name + \".\" + str(i))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKZ9rC0SbEz"
      },
      "outputs": [],
      "source": [
        "img_input = layers.Input(shape = (224,224,3))\n",
        "def custom_model(input):\n",
        "  x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name=\"conv1_pad\")(input)\n",
        "  x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1\")(x)\n",
        "  x = layers.BatchNormalization(axis=3, epsilon=1.00e-5, momentum = 0.9, name=\"bn1\")(x)\n",
        "  x = layers.Activation(\"relu\", name=\"conv1_relu\")(x)\n",
        "  x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=\"pool1_pad\")(x)\n",
        "  x = layers.MaxPooling2D(3, strides=2, name=\"maxpool\")(x)\n",
        "  x = stack_residual_blocks_v1(x, 64, 3, stride1=1, name=\"layer1\")\n",
        "  x = stack_residual_blocks_v1(x, 128, 4, name=\"layer2\")\n",
        "  x = stack_residual_blocks_v1(x, 256, 6, name=\"layer3\")\n",
        "  x = stack_residual_blocks_v1(x, 512, 3, name=\"layer4\")\n",
        "  x = layers.GlobalAveragePooling2D(name=\"avgpool\")(x)\n",
        "  x = layers.Dense(1000, activation='linear', name=\"fc\")(x)\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9jBCPk8SS0x",
        "outputId": "4fffb913-9f53-433a-e8db-e7015bb4c24c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "conv1\n",
            "bn1\n",
            "relu\n",
            "maxpool\n",
            "layer1\n",
            "layer1.0\n",
            "layer1.0.conv1\n",
            "layer1.0.bn1\n",
            "layer1.0.conv2\n",
            "layer1.0.bn2\n",
            "layer1.0.conv3\n",
            "layer1.0.bn3\n",
            "layer1.0.relu\n",
            "layer1.0.downsample\n",
            "layer1.0.downsample.0\n",
            "layer1.0.downsample.1\n",
            "layer1.1\n",
            "layer1.1.conv1\n",
            "layer1.1.bn1\n",
            "layer1.1.conv2\n",
            "layer1.1.bn2\n",
            "layer1.1.conv3\n",
            "layer1.1.bn3\n",
            "layer1.1.relu\n",
            "layer1.2\n",
            "layer1.2.conv1\n",
            "layer1.2.bn1\n",
            "layer1.2.conv2\n",
            "layer1.2.bn2\n",
            "layer1.2.conv3\n",
            "layer1.2.bn3\n",
            "layer1.2.relu\n",
            "layer2\n",
            "layer2.0\n",
            "layer2.0.conv1\n",
            "layer2.0.bn1\n",
            "layer2.0.conv2\n",
            "layer2.0.bn2\n",
            "layer2.0.conv3\n",
            "layer2.0.bn3\n",
            "layer2.0.relu\n",
            "layer2.0.downsample\n",
            "layer2.0.downsample.0\n",
            "layer2.0.downsample.1\n",
            "layer2.1\n",
            "layer2.1.conv1\n",
            "layer2.1.bn1\n",
            "layer2.1.conv2\n",
            "layer2.1.bn2\n",
            "layer2.1.conv3\n",
            "layer2.1.bn3\n",
            "layer2.1.relu\n",
            "layer2.2\n",
            "layer2.2.conv1\n",
            "layer2.2.bn1\n",
            "layer2.2.conv2\n",
            "layer2.2.bn2\n",
            "layer2.2.conv3\n",
            "layer2.2.bn3\n",
            "layer2.2.relu\n",
            "layer2.3\n",
            "layer2.3.conv1\n",
            "layer2.3.bn1\n",
            "layer2.3.conv2\n",
            "layer2.3.bn2\n",
            "layer2.3.conv3\n",
            "layer2.3.bn3\n",
            "layer2.3.relu\n",
            "layer3\n",
            "layer3.0\n",
            "layer3.0.conv1\n",
            "layer3.0.bn1\n",
            "layer3.0.conv2\n",
            "layer3.0.bn2\n",
            "layer3.0.conv3\n",
            "layer3.0.bn3\n",
            "layer3.0.relu\n",
            "layer3.0.downsample\n",
            "layer3.0.downsample.0\n",
            "layer3.0.downsample.1\n",
            "layer3.1\n",
            "layer3.1.conv1\n",
            "layer3.1.bn1\n",
            "layer3.1.conv2\n",
            "layer3.1.bn2\n",
            "layer3.1.conv3\n",
            "layer3.1.bn3\n",
            "layer3.1.relu\n",
            "layer3.2\n",
            "layer3.2.conv1\n",
            "layer3.2.bn1\n",
            "layer3.2.conv2\n",
            "layer3.2.bn2\n",
            "layer3.2.conv3\n",
            "layer3.2.bn3\n",
            "layer3.2.relu\n",
            "layer3.3\n",
            "layer3.3.conv1\n",
            "layer3.3.bn1\n",
            "layer3.3.conv2\n",
            "layer3.3.bn2\n",
            "layer3.3.conv3\n",
            "layer3.3.bn3\n",
            "layer3.3.relu\n",
            "layer3.4\n",
            "layer3.4.conv1\n",
            "layer3.4.bn1\n",
            "layer3.4.conv2\n",
            "layer3.4.bn2\n",
            "layer3.4.conv3\n",
            "layer3.4.bn3\n",
            "layer3.4.relu\n",
            "layer3.5\n",
            "layer3.5.conv1\n",
            "layer3.5.bn1\n",
            "layer3.5.conv2\n",
            "layer3.5.bn2\n",
            "layer3.5.conv3\n",
            "layer3.5.bn3\n",
            "layer3.5.relu\n",
            "layer4\n",
            "layer4.0\n",
            "layer4.0.conv1\n",
            "layer4.0.bn1\n",
            "layer4.0.conv2\n",
            "layer4.0.bn2\n",
            "layer4.0.conv3\n",
            "layer4.0.bn3\n",
            "layer4.0.relu\n",
            "layer4.0.downsample\n",
            "layer4.0.downsample.0\n",
            "layer4.0.downsample.1\n",
            "layer4.1\n",
            "layer4.1.conv1\n",
            "layer4.1.bn1\n",
            "layer4.1.conv2\n",
            "layer4.1.bn2\n",
            "layer4.1.conv3\n",
            "layer4.1.bn3\n",
            "layer4.1.relu\n",
            "layer4.2\n",
            "layer4.2.conv1\n",
            "layer4.2.bn1\n",
            "layer4.2.conv2\n",
            "layer4.2.bn2\n",
            "layer4.2.conv3\n",
            "layer4.2.bn3\n",
            "layer4.2.relu\n",
            "avgpool\n",
            "fc\n"
          ]
        }
      ],
      "source": [
        "for name, layer in torch_model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDg0qp8qSeBU",
        "outputId": "03e9d589-4e2d-43af-b8bf-e470ce819dbe"
      },
      "outputs": [],
      "source": [
        "custom_tf_model = Model(img_input, custom_model(img_input))\n",
        "\n",
        "custom_tf_model.summary()\n",
        "\n",
        "state_dict = torch_model.state_dict()\n",
        "state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDt316CYSj6Y"
      },
      "outputs": [],
      "source": [
        "for layer in custom_tf_model.layers:\n",
        "    if isinstance(layer, layers.Conv2D):\n",
        "        layer.set_weights([state_dict[f'{layer.name}.weight'].numpy().transpose((2, 3, 1, 0))])\n",
        "    elif isinstance(layer, layers.Dense):\n",
        "        layer.set_weights([\n",
        "            state_dict[f'{layer.name}.weight'].numpy().transpose(),\n",
        "            state_dict[f'{layer.name}.bias'].numpy()\n",
        "        ])\n",
        "    elif isinstance(layer, layers.BatchNormalization):\n",
        "        keys = ['weight', 'bias', 'running_mean', 'running_var']\n",
        "        layer.set_weights([state_dict[f'{layer.name}.{key}'].numpy() for key in keys])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE5lnHkESkfX",
        "outputId": "d4bf3f6e-1cd7-4753-e7f2-dc2a30548ca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.053116e-06\n"
          ]
        }
      ],
      "source": [
        "# Compare outputs\n",
        "input_batch = np.random.rand(1, 224, 224, 3).astype(custom_tf_model.dtype)\n",
        "output = custom_tf_model(input_batch).numpy()\n",
        "with torch.no_grad():\n",
        "    torch_output = torch_model(torch.tensor(input_batch.transpose((0, 3, 1, 2)))).numpy()\n",
        "print(np.abs(output - torch_output).max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkQdjLluSoHE",
        "outputId": "2068ace2-6f82-4a4a-c5c1-fcab3a867d49"
      },
      "outputs": [],
      "source": [
        "custom_tf_model.save('resnet50_imagenet.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(custom_tf_model)\n",
        "resnet50_tflite = converter.convert()\n",
        "\n",
        "with open('resnet50_tflite.tflite', 'wb') as f:\n",
        "    f.write(resnet50_tflite)\n",
        "\n",
        "\n",
        "torch.onnx.export(torch_model, torch.tensor(input_batch.transpose((0, 3, 1, 2))), \"../edge/Jetson_Nano/inference/models/resnet50_imagenet.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyQYZi6lTBAL"
      },
      "source": [
        "From the previous models, add a softmax layer and convert to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGwP4UAmSA95"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "import onnxruntime as rt\n",
        "\n",
        "models = ['resnet18_imagenet', 'resnet50_imagenet', 'alexnet_imagenet']\n",
        "\n",
        "for mod in models:\n",
        "  model = tf.keras.models.load_model(\n",
        "      mod + '.h5', custom_objects=None, compile=True, safe_mode=True\n",
        "  )\n",
        "\n",
        "  spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n",
        "\n",
        "  # Add a softmax layer to the model\n",
        "  softmax_layer = tf.keras.layers.Softmax()\n",
        "  output = softmax_layer(model.output)\n",
        "\n",
        "  # Create a new model with the softmax layer\n",
        "  model_with_softmax = tf.keras.Model(inputs=model.input, outputs=output)\n",
        "  preds = model_with_softmax.predict(x)\n",
        "\n",
        "  output_path = mod + \".onnx\"\n",
        "\n",
        "  model_proto, _ = tf2onnx.convert.from_keras(model_with_softmax, input_signature=spec, opset=17, output_path=output_path)\n",
        "  output_names = [n.name for n in model_proto.graph.output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIronz5I5K9M"
      },
      "source": [
        "### Big model RegNetXT for Jetson Orin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-kUhZbZ5RfO"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import regnet_y_32gf, RegNet_Y_32GF_Weights\n",
        "import torch\n",
        "\n",
        "\n",
        "weights = RegNet_Y_32GF_Weights.IMAGENET1K_V2\n",
        "model = regnet_y_32gf(weights=weights)\n",
        "model.eval()\n",
        "\n",
        "# export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels, 224x224 image\n",
        "torch.onnx.export(model, dummy_input, \"../edge/Jetson_Nano/inference/models/regnet.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8YOGWKPTjyQ"
      },
      "source": [
        "## Early Exit for ImageNet models\n",
        "\n",
        "\n",
        "\n",
        "> This was not included in the paper because basic Branchynet implementation lost too much accuracy in complex datasets like ImageNet. This part is kept as a future improvement if techniques like self-distillaition (https://ieeexplore.ieee.org/document/9381661) are used to train the models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no072P3zXWdz"
      },
      "source": [
        "## Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6nuvRxtWgay"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQSO8OYVXG6Y"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(r\"EE_resnet18_two_newweights_4.h5\")\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = Image.open(image).convert('RGB')\n",
        "    image = tf.convert_to_tensor(image)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = (image -mean)/std\n",
        "    image = tf.reshape(image, (1, 224, 224, 3))\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-mW73PGXSTP"
      },
      "outputs": [],
      "source": [
        "# Model for inference using TFLite\n",
        "\n",
        "# Define common model\n",
        "common = Model(inputs=model.input, outputs=model.layers[44].output)\n",
        "\n",
        "# Define branch models\n",
        "branch1 = Model(inputs=model.layers[44].output, outputs=tf.nn.softmax(model.layers[-2].output))\n",
        "branch2 = Model(inputs=model.layers[44].output, outputs=tf.nn.softmax(model.layers[-1].output))\n",
        "\n",
        "\n",
        "# Define custom layer to choose between branches\n",
        "class ChooseBranchLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(ChooseBranchLayer, self).__init__()\n",
        "        self.branch1 = branch1\n",
        "        self.branch2 = branch2\n",
        "\n",
        "    def call(self, inputs):\n",
        "        common_output = inputs\n",
        "        output1 = self.branch1(common_output)\n",
        "        condition = ((tf.reduce_max(output1) > 1.0)) #| (tf.reduce_max(output1) < 0.5))\n",
        "        return tf.cond(condition, lambda: [output1,0], lambda: [self.branch2(common_output),1])\n",
        "\n",
        "# Create input layer\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "# Get common output\n",
        "common_output = common(inputs)\n",
        "\n",
        "# Use the custom layer to choose output based on condition\n",
        "final_output, eexit = ChooseBranchLayer()(common_output)\n",
        "\n",
        "\n",
        "# Define the new model\n",
        "model_for_inference = tf.keras.Model(inputs=inputs, outputs=[final_output, eexit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UasXpjNcXq-B"
      },
      "source": [
        "### Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBEwmiSbXqPJ"
      },
      "outputs": [],
      "source": [
        "def representative_data_generator():\n",
        "    data = tf.data.Dataset.from_tensor_slices(imagenes.astype(np.float32)).batch(1)\n",
        "    for input_value in data:\n",
        "        yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_inference)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_generator\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_scale, input_zero_point = input_details[0]['quantization']\n",
        "output_scale, output_zero_point = output_details[1]['quantization']\n",
        "\n",
        "with open(r\"EE_resnet18_quant_mainbranch.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ClZULy5XO6"
      },
      "source": [
        "# Server Models\n",
        "\n",
        "Here we show how to obtain the models that will run on the server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diMHioAS52sb"
      },
      "source": [
        "## CIFAR-10 :  ViT-H/14\n",
        "\n",
        "We could not find this model pretrained for CIFAR-10 (it is for ImageNet-21k and ImageNet-21k), so we had to fine tune it to CIFAR-10. For this, you can use the official repository:\n",
        "\n",
        "https://github.com/google-research/vision_transformer\n",
        "\n",
        "Which uses JAX. You could also use the following if you prefer PyTorch:\n",
        "\n",
        "https://github.com/bwconrad/vit-finetune\n",
        "\n",
        "\n",
        "If you choose the latter, you can do the following to prepare the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zShKavp--GpN"
      },
      "outputs": [],
      "source": [
        "from src.model import ClassificationModel\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "\n",
        "ckpt_path = \"output/default/version_0/checkpoints/best-step-step=97000-val_acc=0.9847.ckpt\" # Or whatever name your best checkpoint has\n",
        "model = ClassificationModel.load_from_checkpoint(ckpt_path)\n",
        "model = model.to(\"cpu\")\n",
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device=\"cpu\")\n",
        "\n",
        "onnx_model_path = os.path.join(home_dir, \"model/big/vith14.onnx\") # Change this to wherever you want to save the ONNX model\n",
        "torch.onnx.export(model.net,\n",
        "                dummy_input,\n",
        "                onnx_model_path,\n",
        "                export_params=True,\n",
        "                opset_version=17,\n",
        "                do_constant_folding=True\n",
        "                input_names=['input'],\n",
        "                output_names=['output'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kflu-yiW-i3A"
      },
      "outputs": [],
      "source": [
        "!python3 -m onnxruntime.transformers.optimizer --input ${HOME}/model/big/vith14.onnx --output ${HOME}/model/opt/vith14.onnx --hidden_size 1280 --num_heads 16 --opt_level 0 --float16 --use_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s68HccgQ-nke"
      },
      "source": [
        "This will produce a model that behaves well with ONNX runtime CUDA EP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UphsmlG79g-h"
      },
      "source": [
        "## ImageNet-1k :  ConvNeXT (xlarge-sized model)\n",
        "\n",
        "This one is available in the `transformers` module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6KljTdF5ZdD"
      },
      "outputs": [],
      "source": [
        "from transformers import ConvNextForImageClassification,Data2VecVisionForImageClassification\n",
        "import torch\n",
        "\n",
        "torch_model = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-xlarge-224-22k-1k\")\n",
        "\n",
        "torch_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(torch_model,                   # our model to save\n",
        "                  torch_input,                   # model input\n",
        "                  \"../server/convnext.onnx\",     # path to save model\n",
        "                  export_params=True,            # store the trained parameter weights inside the model file\n",
        "                  opset_version=17,              # the ONNX opset version to export the model to\n",
        "                  do_constant_folding=True,      # whether to execute constant folding for optimization\n",
        "                  )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
